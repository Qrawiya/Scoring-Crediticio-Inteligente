# ğŸ§  Modelos DNN y ResNet para ClasificaciÃ³n de Riesgo Crediticio

Este proyecto implementa y compara redes neuronales profundas (DNN) y redes neuronales tipo ResNet para predecir el riesgo crediticio de clientes en base a sus caracterÃ­sticas financieras.

## ğŸ“ Contenido del Proyecto

- **Modelos implementados:**
  - DNN Simple
  - ResNet Simple
  - DNN con HiperparÃ¡metros Optimizado
  - ResNet Optimizado y Ajustado

- **MÃ©tricas evaluadas:**
  - Accuracy
  - AUC (Ãrea Bajo la Curva ROC)
  - Matriz de ConfusiÃ³n
  - Precision, Recall y F1-Score
  - AnÃ¡lisis de errores Tipo I y II
  - Supuesto de AnÃ¡lisis Financiero

---

## ğŸ“Š Resultados Comparativos

| Modelo                   | Accuracy | AUC     |
|--------------------------|----------|---------|
| DNN Simple               | 0.8429   | 0.8997  |
| ResNet Simple            | 0.8071   | 0.8805  |
| DNN Optimizado           | 0.8286   | 0.9110  |
| ResNet Optimizado        | 0.8321   | 0.8970  |

---

## ğŸ” ReflexiÃ³n CrÃ­tica

Se consideran aspectos Ã©ticos como el posible sesgo en los datos histÃ³ricos de aprobaciÃ³n/rechazo. AdemÃ¡s, se evalÃºa la interpretabilidad de los modelos para su aplicaciÃ³n en un contexto real, como en un comitÃ© de riesgo bancario. A pesar de usar redes complejas, se acompaÃ±a con mÃ©tricas claras para comunicar los resultados.

---

## ğŸ’° SuposiciÃ³n Financiera (Ejemplo)

- **Falso Positivo (FP):** Otorgar crÃ©dito a un cliente riesgoso â†’ pÃ©rdida de $5,000
- **Falso Negativo (FN):** Rechazar a un cliente bueno â†’ pÃ©rdida de $1,000

Esto permite estimar el impacto financiero total de las predicciones errÃ³neas por modelo y sugiere que, en un contexto real, ademÃ¡s de accuracy, el costo financiero de los errores debe considerarse al seleccionar el modelo mÃ¡s adecuado.

---

## ğŸ“¦ Requisitos

- Python â‰¥ 3.8
- TensorFlow â‰¥ 2.13
- Optuna (para optimizaciÃ³n)
- scikit-learn
- matplotlib / seaborn

```bash
pip install tensorflow optuna scikit-learn matplotlib seaborn

ğŸ“Œ Autor

Proyecto desarrollado por [Cristobal Araya] como parte de una evaluaciÃ³n de modelos de machine learning aplicados al crÃ©dito. DemostraciÃ³n de habilidades en:

Preprocesamiento y modelado

OptimizaciÃ³n de hiperparÃ¡metros

InterpretaciÃ³n de resultados

Ã‰tica y negocio en IA

ReflexiÃ³n sobre el uso de herramientas de asistencia IA

Este proyecto fue desarrollado con una combinaciÃ³n de criterio propio, conocimiento tÃ©cnico y asistencia con herramientas basadas en inteligencia artificial (como ChatGPT).

Si bien parte del cÃ³digo fue co-creado con soporte de IA, todas las decisiones de diseÃ±o, evaluaciÃ³n, ajustes de modelos, interpretaciÃ³n de resultados y reflexiones fueron definidas por mÃ­ como autor del proyecto.

Este enfoque refleja una prÃ¡ctica profesional moderna en la ciencia de datos, donde se utilizan herramientas avanzadas para mejorar la productividad y el enfoque analÃ­tico, sin reemplazar el juicio humano.

Subo este proyecto con el objetivo de compartir el proceso completo, desde el desarrollo hasta la reflexiÃ³n crÃ­tica, incluyendo tanto los aspectos tÃ©cnicos como Ã©ticos del modelado de riesgos en un contexto financiero.

## ğŸš€ Ideas para Mejorar el Proyecto

Si estÃ¡s interesado en seguir mejorando este proyecto o aprendiendo nuevas herramientas, aquÃ­ tienes algunas sugerencias prÃ¡cticas que puedes implementar. Cada una incluye una breve explicaciÃ³n y cÃ³mo comenzar a aplicarla.

---

### ğŸ“Œ 1. Agregar Interpretabilidad al Modelo

**Â¿QuÃ© hacer?**  
Implementa herramientas de interpretabilidad como **SHAP** o **LIME** para entender cÃ³mo cada variable influye en las predicciones del modelo.

**Â¿Por quÃ© es Ãºtil?**  
Ayuda a comunicar tus resultados a personas no tÃ©cnicas (como un equipo de negocio o comitÃ© de crÃ©dito) y mejora la confianza en el modelo.

**Â¿CÃ³mo empezar?**  
Instala e implementa alguna de estas librerÃ­as:

```bash
pip install shap lime
```

Consulta la documentaciÃ³n de [`shap`](https://github.com/slundberg/shap) y [`lime`](https://github.com/marcotcr/lime) para ver ejemplos aplicados.

---

### ğŸ“Œ 2. Visualizar la Importancia de las Variables

**Â¿QuÃ© hacer?**  
Genera grÃ¡ficos que muestren quÃ© variables tienen mÃ¡s influencia en el modelo, usando:

- Permutation Importance  
- SHAP summary plots

**Â¿Por quÃ© es Ãºtil?**  
Te permite justificar decisiones del modelo y hacer anÃ¡lisis de negocio mÃ¡s claro.

**Â¿CÃ³mo empezar?**  
Usa las funciones de `shap` o librerÃ­as como `eli5` para visualizaciones simples:

```bash
pip install shap eli5
```

---

### ğŸ“Œ 3. Usar Cross-Validation en la OptimizaciÃ³n

**Â¿QuÃ© hacer?**  
En lugar de usar solo un `train/test split`, implementa **k-fold cross-validation** para entrenar y evaluar el modelo.

**Â¿Por quÃ© es Ãºtil?**  
Aumenta la robustez de tus resultados y evita que un split afortunado afecte las mÃ©tricas.

**Â¿CÃ³mo empezar?**  
Si ya estÃ¡s usando `Optuna` o `scikit-learn`, puedes integrar fÃ¡cilmente `StratifiedKFold` en tu pipeline:

```python
from sklearn.model_selection import StratifiedKFold
```

---

### ğŸ“Œ 4. Comparar con Modelos Tradicionales

**Â¿QuÃ© hacer?**  
Agrega un modelo de referencia simple, como `LogisticRegression`, como punto de comparaciÃ³n (**baseline**).

**Â¿Por quÃ© es Ãºtil?**  
A veces los modelos simples ofrecen buen rendimiento y mejor interpretabilidad, lo cual es clave en escenarios reales.

**Â¿CÃ³mo empezar?**  
Usa este modelo con `scikit-learn`:

```bash
pip install scikit-learn
```

```python
from sklearn.linear_model import LogisticRegression
```

---

### ğŸ“Œ 5. Crear un Dashboard con Streamlit

**Â¿QuÃ© hacer?**  
Construye una app web interactiva para que otros puedan probar el modelo cargando sus propios datos.

**Â¿Por quÃ© es Ãºtil?**  
Ideal para entrevistas, mostrar el proyecto en tu portafolio, o presentarlo a usuarios no tÃ©cnicos.

**Â¿CÃ³mo empezar?**

1. Instala Streamlit:
   ```bash
   pip install streamlit
   ```
2. Crea un archivo `app.py` con tu lÃ³gica de predicciÃ³n.
3. Ejecuta la app:
   ```bash
   streamlit run app.py
   ```

---

### ğŸ“Œ 6. Agregar Tests y Validaciones de Calidad

**Â¿QuÃ© hacer?**  
Incluye pequeÃ±as pruebas que aseguren la calidad de los datos de entrada y salida, como:

- Verificar que no haya valores nulos (`NaN`)
- Validar consistencia de las columnas
- Chequear fairness del modelo en distintos grupos

**Â¿Por quÃ© es Ãºtil?**  
Da mayor confianza en el modelo y muestra profesionalismo en el desarrollo.

**Â¿CÃ³mo empezar?**  
Puedes usar `pytest`, o simplemente agregar bloques `assert` en el cÃ³digo:

```python
assert not df.isnull().any().any(), "Existen valores nulos en los datos"
```

---

